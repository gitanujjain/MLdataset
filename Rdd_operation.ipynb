{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitanujjain/MLdataset/blob/master/Rdd_operation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HG1ndKZ-BAKR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar -xf spark-3.5.0-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NpnsUj5YTQnM"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y6YC_aETT5k",
        "outputId": "8a773fed-8078-41ee-89c4-544540acbb10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7b723127a020>\n",
            "<SparkContext master=local appName=Colab>\n"
          ]
        }
      ],
      "source": [
        "print(spark)\n",
        "print(spark.sparkContext)\n",
        "sc=spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RDD METHOD"
      ],
      "metadata": {
        "id": "xNhcSA0tfdeC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1jUBytYYr3V",
        "outputId": "915217ed-5164-48be-a759-a3d8e4ccf999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab\n"
          ]
        }
      ],
      "source": [
        "print(sc.appName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-NMyPByYvMI",
        "outputId": "727418d6-99f9-40f5-8755-aa053e15d4ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# create The RDD\n",
        "n_rdd=sc.range(1,5)\n",
        "n_rdd.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccp6cd6yjcFz",
        "outputId": "9ca3e7b3-8048-4cdc-f489-4d5ad69879fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/sample_data/mnist_test.csv MapPartitionsRDD[5] at textFile at NativeMethodAccessorImpl.java:0\n"
          ]
        }
      ],
      "source": [
        "text_rdd=sc.textFile('/content/sample_data/mnist_test.csv')\n",
        "print(text_rdd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-WwqsYJkVJJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "#sc.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVYEhCFbjxBO"
      },
      "outputs": [],
      "source": [
        "text_rdd2=sc.wholeTextFiles('/content/sample_data/mnist_test.csv')\n",
        "print(text_rdd2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **map(func)**:\n",
        "\n",
        "---\n",
        "Definition: Applies a function to each element in the RDD and returns a new RDD with the results."
      ],
      "metadata": {
        "id": "zO5KZKCgnwY4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uC_2mYhmn_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18f65bfb-60a2-42cf-be2f-edc527cf9a48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 9]\n"
          ]
        }
      ],
      "source": [
        "rdd=sc.parallelize([1,2,3])\n",
        "squrd_rdd=rdd.map(lambda x:x**2)\n",
        "print(squrd_rdd.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**filter(func):**\n",
        "\n",
        "---\n",
        "\n",
        "Definition: Returns a new RDD with elements that satisfy the given predicate (function)."
      ],
      "metadata": {
        "id": "_vMH9i38n8BW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=sc.parallelize(range(1,11))"
      ],
      "metadata": {
        "id": "w_gLzLlPn8cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filter_rdd=rdd.filter(lambda x:x%2==0)\n",
        "print(filter_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQ0RIfFtob6K",
        "outputId": "6565dea1-92b3-456b-bfd2-3eb3a63221f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6, 8, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**flatMap(func):**\n",
        "\n",
        "---\n",
        "Definition: Similar to `map`, but each input item can be mapped to 0 or more output items."
      ],
      "metadata": {
        "id": "CtVfbDF2ozqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_rdd=sc.parallelize(['Hello world', 'Apache Spark'])"
      ],
      "metadata": {
        "id": "1w1ARgG6o7st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flatmap_word_rdd=word_rdd.flatMap(lambda line: line.split(\" \"))\n",
        "print(flatmap_word_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaH_pMLYpVc3",
        "outputId": "a5b80b17-a0b2-4eba-b77e-4a482262bda7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'world', 'Apache', 'Spark']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**distinct(numPartitions=None):**\n",
        "\n",
        "---\n",
        "Definition: Returns a new RDD with distinct elements."
      ],
      "metadata": {
        "id": "jIe2KmR4rseL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([1, 2, 2, 3, 3, 3])\n",
        "distinct_rdd = rdd.distinct()\n",
        "print(distinct_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN3mweelryPk",
        "outputId": "01dd2c24-8703-4635-d5f9-e07b79a21b89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**sample(withReplacement, fraction, seed=None):**\n",
        "\n",
        "---\n",
        "Definition: Returns a random sample of the RDD.\n"
      ],
      "metadata": {
        "id": "LisE9Juyr9Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=sc.parallelize([1,2,3,4,5,5,6])\n",
        "sampler=rdd.sample(withReplacement=False, fraction=0.5)\n",
        "print(sampler.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw5VB_R-r9tS",
        "outputId": "d48c9866-3dda-4639-e50c-bca099632bd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 5, 5, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**union(other):**\n",
        "\n",
        "---\n",
        "Definition: Returns a new RDD that contains the union of the elements in the source RDD and the other RDD."
      ],
      "metadata": {
        "id": "Igzz4QzGtcWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1=sc.parallelize([1,2,3,4,5,6])\n",
        "rdd2=sc.parallelize([11,22,33,44,55])\n",
        "rdd3=rdd1.union(rdd2)\n",
        "print(rdd3.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKkxHzZ2tcwn",
        "outputId": "5112f787-6d99-497a-8467-20a7102c8d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 11, 22, 33, 44, 55]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**intersection(other):**\n",
        "Definition: Returns a new RDD with common elements between the source RDD and the other RDD."
      ],
      "metadata": {
        "id": "G0HIjbffuv7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd4=rdd1.intersection(rdd2)\n",
        "print(rdd4.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-Q5krC7uwm3",
        "outputId": "3173f289-bfd0-41fb-f780-b8f8ca83de45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**subtract(other):**\n",
        "\n",
        "---\n",
        "\n",
        "Definition: Returns a new RDD with elements from the source RDD that are not present in the other RDD."
      ],
      "metadata": {
        "id": "_wW7wXBavUbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd5=rdd1.subtract(rdd2)\n",
        "print(rdd5.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7sXoHRHvUvt",
        "outputId": "ccb496d7-1f53-40e3-89c0-d1aa196dab66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6, 1, 3, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**zip(other):**\n",
        "\n",
        "---\n",
        "Definition: Returns a new RDD by pairing elements from two RDDs."
      ],
      "metadata": {
        "id": "gp8VtdAGv5Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1=sc.parallelize(['a','b','c','d'])\n",
        "rdd2=sc.parallelize([1,2,3,4])\n",
        "rdd6=rdd1.zip(rdd2)\n",
        "print(rdd6.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN9AVrNlwJ0g",
        "outputId": "60dbb5c6-a928-45b5-d872-d19f58f12ccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('a', 1), ('b', 2), ('c', 3), ('d', 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**zipWithIndex():**\n",
        "Definition: Returns a new RDD by adding index to each element of the RDD."
      ],
      "metadata": {
        "id": "Eur6JCxFwvDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=sc.parallelize([\"A\",\"B\",\"C\"])\n",
        "index_rdd=rdd.zipWithIndex()\n",
        "print(index_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLvXt3rC2N5q",
        "outputId": "b90601be-2e32-4b3c-bf9c-29751639b200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', 0), ('B', 1), ('C', 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "u_index_rdd=rdd.zipWithUniqueId()\n",
        "print(u_index_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFBfA59S2d8y",
        "outputId": "c8c0e9bc-d5e7-4ad6-b92d-15aa0c843f8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', 0), ('B', 1), ('C', 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cartesian(other):**\n",
        "\n",
        "---\n",
        "Definition: Returns the Cartesian product of the source RDD and the other RDD."
      ],
      "metadata": {
        "id": "0VQIafKK21e7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1=sc.parallelize([1,2,3])\n",
        "rdd2=sc.parallelize(['A','B'])\n",
        "rdd3=rdd1.cartesian(rdd2)\n",
        "print(rdd3.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRi1Y_Fw-H6D",
        "outputId": "ef003108-680e-49f6-8292-a3a008b0567d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1, 'A'), (1, 'B'), (2, 'A'), (2, 'B'), (3, 'A'), (3, 'B')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**groupByKey(numPartitions=None):**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Definition: Groups elements by key and returns an RDD of `(key, iterable)` pairs."
      ],
      "metadata": {
        "id": "Cak-HBXF_Y-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=[('A',1),('B',2),('C',3),('A',2),('A',3),('C',3)]\n",
        "rdd_data=sc.parallelize(data)\n",
        "grouped_rdd=rdd_data.groupByKey()\n",
        "print(grouped_rdd.collect())\n",
        "print(grouped_rdd.mapValues(list).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEHMOSD6_Xjj",
        "outputId": "2847b1e7-accf-413f-a3db-fd50c97d2f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', <pyspark.resultiterable.ResultIterable object at 0x7aaf9622a920>), ('B', <pyspark.resultiterable.ResultIterable object at 0x7aaf96228c10>), ('C', <pyspark.resultiterable.ResultIterable object at 0x7aaf9622a1d0>)]\n",
            "[('A', [1, 2, 3]), ('B', [2]), ('C', [3, 3])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**reduceByKey(func, numPartitions=None):**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Definition: Aggregates values of each key using a specified function."
      ],
      "metadata": {
        "id": "xTKxSJp8AqWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_rdd=rdd_data.reduceByKey(lambda a,b:a+b)\n",
        "print(reduce_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O5QlQ-3Aow1",
        "outputId": "4429e62d-d879-4ec7-ad1a-c568a1a9f9e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', 6), ('B', 2), ('C', 6)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sortByKey(ascending=True, numPartitions=None):\n",
        "Definition: Sorts the RDD by key.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9Tkb51sHnNiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=[(\"A\",1),(\"B\",3), (\"C\",0)]\n",
        "rdd_1=sc.parallelize(data)\n",
        "sorted_rdd=rdd_1.sortByKey()\n",
        "print(sorted_rdd.collect())"
      ],
      "metadata": {
        "id": "doxc3kDCnMol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3c6d11d-2c69-4ebc-8c80-e4b3a939eeb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', 1), ('B', 3), ('C', 0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**mapPartitions(func):**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Definition: Applies a function to each partition of the RDD."
      ],
      "metadata": {
        "id": "HKKK67t4dUGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def partition_sum(iterator):\n",
        "  yield sum(iterator)\n",
        "\n",
        "rdd_2=sc.parallelize([1,2,3,4,5],3)\n",
        "partition_rdd=rdd_2.mapPartitions(partition_sum)\n",
        "partition_rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlyB18dHdgfU",
        "outputId": "3cbc6b50-e25b-4243-9945-879e23efb91a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 5, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "mapPartitionsWithIndex(func):\n",
        "Definition: Applies a function to each partition of the RDD with index."
      ],
      "metadata": {
        "id": "87lBzWlCgAi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def partition_sum_with_index(index, iterator):\n",
        "  yield index , sum(iterator)\n",
        "\n",
        "rdd_3=sc.parallelize([1,2,3,4,5,6],2)\n",
        "partition_index_sum=rdd_3.mapPartitionsWithIndex(partition_sum_with_index)\n",
        "partition_index_sum.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGd96dFZf9Fu",
        "outputId": "945963bd-76d0-4920-e962-2033ce3f9e62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 6), (1, 15)]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**sample(withReplacement, fraction, seed=None)**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Definition: Returns a random sample of the RDD."
      ],
      "metadata": {
        "id": "MT-sMOwhm03s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=sc.parallelize([1,2,3,4,5,6,7])\n",
        "sample_rdd=rdd.sample(withReplacement=False, fraction=0.5)\n",
        "print(sample_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aN48T-YwkcM",
        "outputId": "4e45dd08-1e4d-404c-d0fc-8ba75b350218"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 4, 5, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**distinct(numPartitions=None):**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Definition: Returns a new RDD with distinct elements."
      ],
      "metadata": {
        "id": "hkPphzPow7Gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=sc.parallelize([1,2,3,4,2,3,4,5,6,5,3,2,2,1,1,3,3])\n",
        "distinct_rdd=rdd.distinct()\n",
        "print(distinct_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnJe06QpxB_r",
        "outputId": "42829c0b-9928-427b-9d02-82143a28bf34"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**groupByKey(numPartitions=None):**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Definition: Groups elements by key and returns an RDD of `(key, iterable)` pairs."
      ],
      "metadata": {
        "id": "ZfM08NPxxY1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"A\", 1),(\"B\", 2), (\"A\",3), (\"C\", 4)]\n",
        "rdd=sc.parallelize(data)\n",
        "grouped_rdd=rdd.groupByKey()\n",
        "print(grouped_rdd.mapValues(list).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pF8J1RazE4V",
        "outputId": "9fe7d96e-ddd3-43ae-8ad8-f5558229a950"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', [1, 3]), ('B', [2]), ('C', [4])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**reduceByKey(func, numPartitions=None):**\n",
        "\n",
        "---\n",
        "\n",
        "Definition: Aggregates values of each key using a specified function."
      ],
      "metadata": {
        "id": "VB0StiVS0C9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_rdd=rdd.reduceByKey(lambda a,b:a+b)\n",
        "print(reduce_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP0B-vfk0USI",
        "outputId": "91195700-365e-4bc6-c7d9-c118556ebd10"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', 4), ('B', 2), ('C', 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**sortByKey(ascending=True, numPartitions=None):**\n",
        "\n",
        "---\n",
        "\n",
        "Definition: Sorts the RDD by key."
      ],
      "metadata": {
        "id": "hMMaeBui0slF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data =[(\"B\",2),(\"A\",1),(\"C\",3)]\n",
        "rdd= sc.parallelize(data)\n",
        "sorted_rdd=rdd.sortByKey()\n",
        "print(sorted_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoR5tmzv01Of",
        "outputId": "41c1e9e2-5100-4bec-de20-cea3584b7b14"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', 1), ('B', 2), ('C', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** coalesce(numPartitions):**\n",
        "\n",
        "---\n",
        "\n",
        "Definition: Reduces the number of partitions in the RDD to the specified number."
      ],
      "metadata": {
        "id": "oj_jFZi_2SXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=sc.parallelize([1,2,3,4,5,6,7,8,9,0],5)\n",
        "coll_rdd= rdd.coalesce(2)\n",
        "print(coll_rdd.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAbxGU4a2WN5",
        "outputId": "9810553a-c36e-4bb3-cd81-e7402262e246"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**repartition(numPartitions):**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Definition: Reshuffles the data in the RDD and creates the specified number of partitions."
      ],
      "metadata": {
        "id": "J3nvxQ9a3KZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=sc.parallelize([1,2,3,4,5,6,7,8,9],2)\n",
        "repa_rdd=rdd.repartition(4)\n",
        "print(repa_rdd.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NmSoSOs3JL-",
        "outputId": "d85c267a-d01f-4e3d-cb63-eb2976db12cb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**mapValues(func):**\n",
        "\n",
        "---\n",
        "Definition: Applies a function to the values of each key-value pair in the RDD."
      ],
      "metadata": {
        "id": "NOf9GZB232Q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=[(\"A\",1),(\"B\",2),(\"A\",4)]\n",
        "rdd=sc.parallelize(data)\n",
        "map_vale_rdd=rdd.mapValues(lambda x: x+10)\n",
        "print(map_vale_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gG0oDPN472U",
        "outputId": "035846db-c760-4c31-b40f-12abf771a715"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', 11), ('B', 12), ('A', 14)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**flatMapValues(func):**\n",
        "\n",
        "---\n",
        "Definition: Similar to `mapValues`, but each input item can be mapped to 0 or more output items."
      ],
      "metadata": {
        "id": "5eY9XuPV5qzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=[(\"A\",[1,2]), (\"B\",[3]),(\"A\",[4,5])]\n",
        "rdd=sc.parallelize(data)\n",
        "new_rdd=rdd.flatMapValues(lambda x:x)\n",
        "print(new_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1lG7XYP56b5",
        "outputId": "eb00da66-429e-4527-a64c-647a01202782"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', 1), ('A', 2), ('B', 3), ('A', 4), ('A', 5)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "aggregateByKey(zeroValue, seqFunc, combFunc, numPartitions=None):\n",
        "Definition: Aggregates the values of each key using a given function."
      ],
      "metadata": {
        "id": "uEhXfp8D7Vc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=[(\"A\",1),(\"A\",2),(\"B\",1),('B',2),('C',3)]\n",
        "rdd=sc.parallelize(data)\n",
        "aggregated_rdd=rdd.aggregateByKey(lambda x:(x,1),\n",
        "                                  lambda acc,value:(acc[0]+value,acc[1]+1),\n",
        "                                  lambda acc1,acc2:(acc1[0]+acc2[0],acc1[1]+acc2[1]))\n"
      ],
      "metadata": {
        "id": "e8IiHy157Sjp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions=None):**\n",
        "\n",
        "---\n",
        "\n",
        "Definition: Aggregates values of each key by first applying a combine function per partition, and then combining the results across partitions."
      ],
      "metadata": {
        "id": "F0o3QAbItoz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [('A',1),('B',2),('A',3),('C',4)]\n",
        "rdd= sc.parallelize(data)\n",
        "combine_data=rdd.combineByKey(lambda x: (x,1), lambda acc, value:(acc[0]+value,acc[1]+1),lambda acc1, acc2:(acc1[0]+acc2[0],acc1[1]+acc2[1]))\n",
        "print(combine_data.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrXP_gqRtzRh",
        "outputId": "abe89206-d725-4832-faf1-7a3ac63e551d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', (4, 2)), ('B', (2, 1)), ('C', (4, 1))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**pipe(command, env=None):**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Definition: Pipes the elements of the RDD through an external command and returns the output as a new RDD."
      ],
      "metadata": {
        "id": "gbpzsUP1zqdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=sc.parallelize([\"Hello\",\"pyspark\"])\n",
        "pipe_rdd=rdd.pipe(\"appche pyspark\")\n",
        "print(pipe_rdd.collect())"
      ],
      "metadata": {
        "id": "dBeXntXFz2lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**glom():**\n",
        "\n",
        "---\n",
        "Definition: Returns an RDD where each partition is a list of elements."
      ],
      "metadata": {
        "id": "eu5aErmhypt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=sc.parallelize([1,2,3,4,5],2)\n",
        "glom_rdd=rdd.glom()\n",
        "print(glom_rdd.collect())"
      ],
      "metadata": {
        "id": "9ABRjSuCw6mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**keyBy(f):**\n",
        "\n",
        "---\n",
        "Definition: Returns a new RDD by creating tuples where the first element is the result of applying the function to each element in the RDD, and the second element is the original element"
      ],
      "metadata": {
        "id": "kBNXp_TsxHL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=sc.parallelize([\"A\",\"B\",\"C\"])\n",
        "key_by_rdd=rdd.keyBy(lambda x: x.lower())\n",
        "print(key_by_rdd.collect())"
      ],
      "metadata": {
        "id": "pnJsKGbJxxbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e44d17qONLqD"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.types import LongType"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ9mXc10AkyV"
      },
      "source": [
        "**1. Create Empty RDD in PySpark** : there are two method\n",
        "\n",
        "\n",
        "*   emptyRDD()\n",
        "*   parallelize()\n",
        "\n",
        "*Note: If you try to perform operations on empty RDD you going to get ValueError(\"RDD is empty\").*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnyMIRwVJxiP"
      },
      "outputs": [],
      "source": [
        "emptyRDD = spark.sparkContext.emptyRDD()\n",
        "print(emptyRDD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDHNK4lL-z1C"
      },
      "outputs": [],
      "source": [
        "rdd2= spark.sparkContext.parallelize(['Hello'])\n",
        "print(rdd2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h34Ika-n7E-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQKZf5nTn7MU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y28HOt6VBjeX"
      },
      "source": [
        "2. Create Empty DataFrame with Schema (StructType)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzsLA71N_LkI"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType, StructType, StringType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDqGtDAVB6fz"
      },
      "outputs": [],
      "source": [
        "schema = StructType([StructField(\"firstname\",StringType(),True),StructField(\"middlename\", StringType(), True), StructField(\"lastname\", StringType(), True)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rskANr3dC0Pr"
      },
      "outputs": [],
      "source": [
        "#Now use the empty RDD created above and pass it to createDataFrame() of SparkSession along with the schema for column names & data types.\n",
        "emptyDF=spark.createDataFrame(emptyRDD, schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0FUBvu_FtEZ"
      },
      "outputs": [],
      "source": [
        "emptyDF.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7xVZaEUGDtS"
      },
      "source": [
        "**3. Convert Empty RDD to DataFrame**: create empty DataFrame by converting empty RDD to DataFrame using toDF()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ie_Ouy8lF5F8"
      },
      "outputs": [],
      "source": [
        "emptyDF1=emptyRDD.toDF(schema)\n",
        "emptyDF1.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXbI7N3CINRu"
      },
      "source": [
        "4. Create Empty DataFrame with Schema.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZaAxR25IJO1"
      },
      "outputs": [],
      "source": [
        "df1=spark.createDataFrame([], schema)\n",
        "df1.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJbA8VgtIpTU"
      },
      "source": [
        "**5. Create Empty DataFrame without Schema (no columns)**: To create empty DataFrame with out schema (no columns) just create a empty schema and use it while creating PySpark DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHBmmIn2IfKd"
      },
      "outputs": [],
      "source": [
        "df2=spark.createDataFrame([], StructType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLmkjdfDJAGg"
      },
      "outputs": [],
      "source": [
        "df2.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eqn98bJc7Kc"
      },
      "source": [
        "**PySpark Replace Empty Value With None/null on DataFrame**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vu7D7zDGc6Mb"
      },
      "outputs": [],
      "source": [
        "data=[(\"anuj\",\"\"),(\"\", \"delhi\"),(\"Arham\",\"Ahemmadabad\"),(\"\",\"Jammu\")]\n",
        "df1=spark.createDataFrame(data,[\"name\",\"location\"])\n",
        "df1.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3ovwbkHgFw8"
      },
      "source": [
        "**PySpark Replace Empty Value with None**: In order to replace empty value with None/null on single DataFrame column, you can use withColumn() and when().otherwise() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RX2Vsfzgnl_"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, when"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CwkdFCvg5z5"
      },
      "outputs": [],
      "source": [
        "df1.withColumn(\"name\",when(col(\"name\")==\"\", None).otherwise(col(\"name\"))).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnRDw7VBh04r"
      },
      "source": [
        "***Replace Empty Value with None on All DataFrame Columns***\n",
        "To replace an empty value with None/null on all DataFrame columns, use df.columns to get all DataFrame columns, loop through this by applying conditions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hopYF285ezuq"
      },
      "outputs": [],
      "source": [
        "df2=df1.select([when(col(c)==\"\",None).otherwise(col(c)).alias(c) for c in df1.columns])\n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN7_t-BikCjZ"
      },
      "source": [
        "**Replace Empty Value with None on Selected Columns**\n",
        "Similarly, you can also replace a selected list of columns, specify all columns you wanted to replace in a list and use this on same expression above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsTpFF8_kaa6"
      },
      "outputs": [],
      "source": [
        "replce_col=[\"location\"]\n",
        "df3=df1.select([when(col(c)==\"\", None).otherwise(col(c)).alias(c) for c in replce_col])\n",
        "df3.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8sgCMhvj3y4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq5uxnam3sk1"
      },
      "source": [
        "**PySpark When Otherwise | SQL Case When Usage**: PySpark supports a way to check multiple conditions in sequence and returns a value when the first condition met by using SQL like case when and when().otherwise() expressions, these works similar to “Switch\" and \"if then else\" statements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWcZbz4V4Fv8"
      },
      "source": [
        "PySpark When Otherwise – when() is a SQL function that returns a Column type and otherwise() is a function of Column, if otherwise() is not used, it returns a None/NULL value.\n",
        "\n",
        "PySpark SQL Case When – This is similar to SQL expression, Usage: CASE WHEN cond1 THEN result WHEN cond2 THEN result... ELSE result END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tIJ3-OOjuZG"
      },
      "outputs": [],
      "source": [
        "data=[(\"james\",\"M\",6000),(\"Michael\",\"M\",70000),(\"Robert\",None,400000),(\"Maria\",\"F\",500000),(\"jen\",\"\",None)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jisGxo4p40I6"
      },
      "outputs": [],
      "source": [
        "columns=[\"name\",\"gender\",\"salary\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9TMt20o40VE"
      },
      "outputs": [],
      "source": [
        "df=spark.createDataFrame(data=data,schema=columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qI4xZMdu50Zn"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujPM52u86Eve"
      },
      "source": [
        "1. Using when() otherwise() on PySpark DataFrame.\n",
        "PySpark when() is SQL function, in order to use this first you should import and this returns a Column type, otherwise() is a function of Column, when otherwise() not used and none of the conditions met it assigns None (Null) value. Usage would be like when(condition).otherwise(default).\n",
        "\n",
        "when() function take 2 parameters, first param takes a condition and second takes a literal value or Column, if condition evaluates to true then it returns a value from second param.\n",
        "\n",
        "The below code snippet replaces the value of gender with a new derived value, when conditions not matched, we are assigning “Unknown” as value, for null assigning empty.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrYDneds53AY"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8hc8kIV6QS4"
      },
      "outputs": [],
      "source": [
        "df2=df.withColumn(\"New_column\", when(df.gender == \"M\",\"MALE\")\\\n",
        "                              .when(df.gender == \"F\",\"FEMALE\")\\\n",
        "                              .when(df.gender.isNull(),\"\")\\\n",
        "                              .otherwise(df.gender))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2u5TMCY17CJ2"
      },
      "outputs": [],
      "source": [
        "df2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgbDZxbU7DoL"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "df2= df.select(col(\"*\"), when(df.gender == \"M\",\"Male\").when(df.gender==\"F\",\"Female\").when(df.gender.isNull(),\"\").otherwise(df.gender).alias(\"new_gender\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYVYZntuB8LW"
      },
      "source": [
        "**2. PySpark SQL Case When on DataFrame**.\n",
        "If you have a SQL background you might have familiar with Case When statement that is used to execute a sequence of conditions and returns a value when the first condition met, similar to SWITH and IF THEN ELSE statements. Similarly, PySpark SQL Case When statement can be used on DataFrame, below are some of the examples of using with withColumn(), select(), selectExpr() utilizing expr() function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeNvCYwSCGhf"
      },
      "source": [
        "Syntax of SQL CASE WHEN ELSE END\n",
        "\n",
        "\n",
        "CASE\n",
        "    WHEN condition1 THEN result_value1\n",
        "    WHEN condition2 THEN result_value2\n",
        "    -----\n",
        "    -----\n",
        "    ELSE result\n",
        "END;\n",
        "\n",
        "*   CASE is the start of the expression\n",
        "*   Clause WHEN takes a condition, if condition true it returns a value from THEN\n",
        "*   If the condition is false it goes to the next condition and so on.\n",
        "*   If none of the condition matches, it returns a value from the ELSE clause.\n",
        "\n",
        "*   END is to end the expression.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoSzyMYU7t9l"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import expr, col\n",
        "df3=df.withColumn(\"new_gender\", expr(\"case when gender='M' then 'Male' when gender='F' then 'female' when gender is NULL then '' else gender end\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-Bca2LaDfMU"
      },
      "outputs": [],
      "source": [
        "df3.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKeXIxWbDkG8"
      },
      "outputs": [],
      "source": [
        "df4= df.select(col(\"*\"), expr(\"case when gender='M' then 'Male' when gender='F' then 'female' when gender is NULL then '' else gender end\").alias(\"new_g\"))\n",
        "df4.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEuOC1tMEShS"
      },
      "source": [
        "**2.2 Using Case When on SQL Expression**\n",
        "You can also use Case When with SQL statement after creating a temporary view. This returns a similar output as above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agVI645ED58t"
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"EMP\")\n",
        "spark.sql(\"select name , case when gender ='M' then 'MALE'\"\\\n",
        "          \"when gender ='F' then 'female'\"\\\n",
        "          \"when gender is null then '' \"\\\n",
        "          \"else gender end as new_gender from emp\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDMf-WmdKXnU"
      },
      "source": [
        "**2.3. Multiple Conditions using & and | operator**\n",
        "We often need to check with multiple conditions, below is an example of using PySpark When Otherwise with multiple conditions by using and (&) or (|) operators. To explain this I will use a new set of data to make it simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83NumX_dneSc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEZgGTCSmpBb"
      },
      "source": [
        "PySpark SQL expr() (Expression) Function\n",
        "\n",
        "PySpark expr() is a SQL function to execute SQL-like expressions and to use an existing DataFrame column value as an expression argument to Pyspark built-in functions.\n",
        "Most of the commonly used SQL functions are either part of the PySpark Column class or built-in pyspark.\n",
        "sql.functions API, besides these PySpark also supports many other SQL functions,\n",
        "so in order to use these, you have to use expr() function.\n",
        "\n",
        "Below are 2 use cases of PySpark expr() funcion.\n",
        "\n",
        "First, allowing to use of SQL-like functions that are not present in PySpark Column type & pyspark.sql.functions API. for example CASE WHEN, regr_count().\n",
        "Second, it extends the PySpark SQL Functions by allowing to use DataFrame columns in functions for expression. for example, if you wanted to add a month value from a column to a Date column. I will explain this in the example below.\n",
        "**bold text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOqjjVWLJ3vA"
      },
      "outputs": [],
      "source": [
        "data=[(\"James\",\"Bond\"),(\"AMuj\",\"jain\")]\n",
        "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zm6m-yhoJBw"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xho26dcjoXCp"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "df.withColumn(\"full name\", expr(\"col1 || ',' || col2\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "am0SFRqfpKUv"
      },
      "outputs": [],
      "source": [
        "data =[(\"James\",\"M\"),(\"Michael\",\"F\"),(\"Jen\",\"\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J3W9jsRqSD8"
      },
      "outputs": [],
      "source": [
        "columns=[\"Name\",\"gender\"]\n",
        "df=spark.createDataFrame(data, schema=columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9yjo6l6quq3"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sVqsIm9qwBQ"
      },
      "outputs": [],
      "source": [
        "df2=df.withColumn(\"gender\", expr(\"case when gender='M' then 'male' when gender='F' then 'Female' else 'unknown' end \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ni3WVMKLruod"
      },
      "outputs": [],
      "source": [
        "df2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4xTKDpHrwLI"
      },
      "outputs": [],
      "source": [
        "data2=[(\"2013-01-01\",1),(\"2010-01-04\",2),(\"2013-01-05\",3)]\n",
        "df=spark.createDataFrame(data2).toDF(\"date\",\"increment\")\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2otqqfCvknj"
      },
      "outputs": [],
      "source": [
        "df.select(df.date, df.increment,expr(\"add_months(date,increment)\").alias(\"inc_date\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrsnFrwqwPFx"
      },
      "outputs": [],
      "source": [
        "df.select(df.date, df.increment,expr(\"add_months(date,increment) as inc_date\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDRM8_SjwtDs"
      },
      "outputs": [],
      "source": [
        "df.select(\"increment\", expr(\"cast(increment as string) as str_increment\")).printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAYlP39txBxl"
      },
      "outputs": [],
      "source": [
        "df.select(df.date, df.increment,expr(\"increment + 5 as new_increment\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QraRoeaixjj0"
      },
      "outputs": [],
      "source": [
        "data=[(200,200),(500,500),(300,200)]\n",
        "df=spark.createDataFrame(data, schema=('col1','col2'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f_yK_HUyaAR"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QM8OhQRmybqu"
      },
      "outputs": [],
      "source": [
        "df.select(df.col1,df.col2,expr(\"col1==col2\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0h3XeGyyvB9"
      },
      "outputs": [],
      "source": [
        "df.filter(expr(\"col1==col2\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MHCPxdM0Cf5"
      },
      "source": [
        "**PySpark lit() – Add Literal or Constant to DataFrame**\n",
        "PySpark SQL functions lit() and typedLit() are used to add a new column to DataFrame by assigning a literal or constant value. Both these functions return Column type as return type.\n",
        "\n",
        "Both of these are available in PySpark by importing pyspark.sql.functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5hpi6ixy3tS"
      },
      "outputs": [],
      "source": [
        "data=[(\"111\", 50000),(\"112\",50001),(\"113\", 50003),(\"114\",50004)]\n",
        "columns=[\"empid\",\"salary\"]\n",
        "df=spark.createDataFrame(data, schema=columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUC-WjNo1SpC"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxDzrrVs1T4T"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col,lit\n",
        "df2=df.select(col(\"empid\"), col(\"salary\"), lit(\"1\").alias(\"lit_content\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dH5YN3E_16h8"
      },
      "outputs": [],
      "source": [
        "df2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz0D5K5q18wz"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when, lit, col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w60Uf3GB4E1q"
      },
      "outputs": [],
      "source": [
        "df3=df2.withColumn(\"lit_value2\", when((col(\"Salary\")>=40000) & (col(\"Salary\")<=50000),lit(\"100\")).otherwise(lit(\"200\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ccy4mfJ54vcv"
      },
      "outputs": [],
      "source": [
        "df3.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTSG1MxR47h5"
      },
      "source": [
        "**typedLit() Function** – Syntax\n",
        "Difference between lit() and typedLit() is that, typedLit function can handle collection types e.g.: Array, Dictionary(map) e.t.c. Unfortunately, I could not find this function in PySpark, when I find it, I will add an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVcTbzU5512a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo8k39egBdve"
      },
      "source": [
        "# **Explode**\n",
        "The `explode` function helps you to transforming arrays or maps within a column into separate rows, creating a more granular view of your data. It transforms each element of an array or each key-value pair of a map into a separate row, making it a must-know for efficient data manipulation.\n",
        "\n",
        "**Syntex:**\n",
        "```\n",
        "df_expanded = df.select(\"id\", explode(\"your_array_column\").alias(\"exploded_column\"))\n",
        "```\n",
        "\n",
        "𝑾𝒉𝒚 𝑼𝒔𝒆 `𝒆𝒙𝒑𝒍𝒐𝒅𝒆`?\n",
        "- 𝐹𝑙𝑎𝑡𝑡𝑒𝑛𝑖𝑛𝑔 𝐴𝑟𝑟𝑎𝑦𝑠: Ideal for scenarios where you want to transform arrays into individual rows.\n",
        "- 𝐻𝑎𝑛𝑑𝑙𝑖𝑛𝑔 𝑁𝑒𝑠𝑡𝑒𝑑 𝐷𝑎𝑡𝑎: Perfect for working with complex, nested structures common in real-world datasets.\n",
        "\n",
        "𝑯𝒐𝒘 𝑪𝒂𝒏 𝒀𝒐𝒖 𝑳𝒆𝒗𝒆𝒓𝒂𝒈𝒆 𝑰𝒕?\n",
        "- 𝑁𝑒𝑠𝑡𝑒𝑑 𝐽𝑆𝑂𝑁 𝑃𝑟𝑜𝑐𝑒𝑠𝑠𝑖𝑛𝑔: Unpack nested JSON arrays or maps for analysis.\n",
        "- 𝐷𝑎𝑡𝑎 𝑁𝑜𝑟𝑚𝑎𝑙𝑖𝑧𝑎𝑡𝑖𝑜𝑛: Flatten arrays to simplify downstream processing.\n",
        "- 𝐸𝑥𝑝𝑙𝑜𝑟𝑎𝑡𝑜𝑟𝑦 𝐷𝑎𝑡𝑎 𝐴𝑛𝑎𝑙𝑦𝑠𝑖𝑠: Gain insights from nested structures with ease.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6egWKc2g4w6j",
        "outputId": "89d3bf46-a82b-45d8-e5e2-9ff7ab726f6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------------+\n",
            "| id|     numbers|\n",
            "+---+------------+\n",
            "|  1|[10, 20, 30]|\n",
            "|  2|    [40, 50]|\n",
            "|  3|        [60]|\n",
            "+---+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import explode\n",
        "\n",
        "# Sample data\n",
        "data = [(1, [10, 20, 30]), (2, [40, 50]), (3, [60])]\n",
        "\n",
        "# Create a DataFrame and showing raw data\n",
        "df = spark.createDataFrame(data, [\"id\", \"numbers\"])\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vro2VZaaCOOl",
        "outputId": "52a0733a-874b-41fc-c7a2-1671706e2323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+\n",
            "| id|number|\n",
            "+---+------+\n",
            "|  1|    10|\n",
            "|  1|    20|\n",
            "|  1|    30|\n",
            "|  2|    40|\n",
            "|  2|    50|\n",
            "|  3|    60|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Use explode for arrays\n",
        "df_expanded = df.select(\"id\", explode(\"numbers\").alias(\"number\"))\n",
        "\n",
        "# Show the result\n",
        "df_expanded.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ph8Z-5O79RFz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaHP69cp9RT5"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3kXuhvq9SEn"
      },
      "outputs": [],
      "source": [
        "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
        "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
        "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
        "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
        "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
        "  ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xkf5QvHD9aQk"
      },
      "outputs": [],
      "source": [
        "schema = StructType([ \\\n",
        "    StructField(\"firstname\",StringType(),True), \\\n",
        "    StructField(\"middlename\",StringType(),True), \\\n",
        "    StructField(\"lastname\",StringType(),True), \\\n",
        "    StructField(\"id\", StringType(), True), \\\n",
        "    StructField(\"gender\", StringType(), True), \\\n",
        "    StructField(\"salary\", IntegerType(), True) \\\n",
        "  ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KphZ7oEz9dJX"
      },
      "outputs": [],
      "source": [
        "df = spark.createDataFrame(data=data,schema=schema)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q82ziWjruuaj",
        "outputId": "6dd4eeea-217e-4e30-aa49-41cc2a72e1c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Row(firstname='James', middlename='', lastname='Smith', id='36636', gender='M', salary=3000)\n",
            "Row(firstname='Michael', middlename='Rose', lastname='', id='40288', gender='M', salary=4000)\n",
            "Row(firstname='Robert', middlename='', lastname='Williams', id='42114', gender='M', salary=4000)\n",
            "Row(firstname='Maria', middlename='Anne', lastname='Jones', id='39192', gender='F', salary=4000)\n",
            "Row(firstname='Jen', middlename='Mary', lastname='Brown', id='', gender='F', salary=-1)\n"
          ]
        }
      ],
      "source": [
        "for each in df.collect():\n",
        "  print(each)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vv_vD9ME9iuG",
        "outputId": "a699801c-8440-4748-8f64-9008fb4f74e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTjHmruj9mXn",
        "outputId": "7d5d632b-3146-49eb-fe99-0239be13a30d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'type': 'struct',\n",
              " 'fields': [{'name': 'firstname',\n",
              "   'type': 'string',\n",
              "   'nullable': True,\n",
              "   'metadata': {}},\n",
              "  {'name': 'middlename', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
              "  {'name': 'lastname', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
              "  {'name': 'id', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
              "  {'name': 'gender', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
              "  {'name': 'salary', 'type': 'integer', 'nullable': True, 'metadata': {}}]}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.schema.jsonValue()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlkjYoFW9sgQ"
      },
      "outputs": [],
      "source": [
        "schemaFromJson = StructType.fromJson(df.schema.jsonValue())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xERXZUmRJBG4",
        "outputId": "7b581b6f-660c-47f4-f24e-54b1d9474bef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "StructType([StructField('firstname', StringType(), True), StructField('middlename', StringType(), True), StructField('lastname', StringType(), True), StructField('id', StringType(), True), StructField('gender', StringType(), True), StructField('salary', IntegerType(), True)])\n"
          ]
        }
      ],
      "source": [
        "print(schemaFromJson)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivIvVm8WJMwH"
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"source_update\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHjZd0XzIZq9",
        "outputId": "8525c130-aee4-42cc-a91d-f475969e2fdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+--------+-----+------+------+\n",
            "|firstname|middlename|lastname|   id|gender|salary|\n",
            "+---------+----------+--------+-----+------+------+\n",
            "|    James|          |   Smith|36636|     M|  3000|\n",
            "|  Michael|      Rose|        |40288|     M|  4000|\n",
            "|   Robert|          |Williams|42114|     M|  4000|\n",
            "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
            "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
            "+---------+----------+--------+-----+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"select * from source_update\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V49NK5CUIgOO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4fIpDnbyqdp"
      },
      "source": [
        "**Sapient Test**\n",
        "\n",
        "I have a data set which contains fields such as: item, event, timestamp, userid while lacking of the sessionId.\n",
        "I'm expected to create a session _id which expires for every 30 minutes window.\n",
        "This session _id should be unique per session per user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSwo-UlNyvJQ"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5dAS12xy39w",
        "outputId": "d5bf00ce-0719-454b-dfcb-1d864fb603dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------+-------------+------+\n",
            "| item|   event|    timestamp|userId|\n",
            "+-----+--------+-------------+------+\n",
            "| blue|    view|1610494094750|    11|\n",
            "|green|addtobag|1510593114350|    21|\n",
            "|  red|   close|1610493115350|    41|\n",
            "| blue|    view|1610494094350|    11|\n",
            "| blue|   close|1510593114312|    21|\n",
            "|  red|    view|1610493114350|    41|\n",
            "|  red|    view|1610593114350|    41|\n",
            "|green|purchase|1610494094350|    31|\n",
            "+-----+--------+-------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "df=spark.createDataFrame([\n",
        "(\"blue\",\"view\",1610494094750,11),\n",
        "(\"green\",\"addtobag\",1510593114350,21),\n",
        "(\"red\",\"close\",1610493115350,41),\n",
        "(\"blue\",\"view\",1610494094350,11),\n",
        "(\"blue\",\"close\",1510593114312,21),\n",
        "(\"red\",\"view\",1610493114350,41),\n",
        "(\"red\",\"view\",1610593114350,41),\n",
        "(\"green\",\"purchase\",1610494094350,31)\n",
        "],[\"item\",\"event\",\"timestamp\",\"userId\"])\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbmk7AX6y6pA",
        "outputId": "73907b24-89ae-4b49-dfd7-826e9ab1ce97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------+-------------+------+-------------+\n",
            "| item|   event|    timestamp|userId|   session_id|\n",
            "+-----+--------+-------------+------+-------------+\n",
            "| blue|    view|1610494094350|    11|         NULL|\n",
            "| blue|    view|1610494094750|    11|1610494094350|\n",
            "| blue|   close|1510593114312|    21|         NULL|\n",
            "|green|addtobag|1510593114350|    21|1510593114312|\n",
            "|green|purchase|1610494094350|    31|         NULL|\n",
            "|  red|    view|1610493114350|    41|         NULL|\n",
            "|  red|   close|1610493115350|    41|1610493114350|\n",
            "|  red|    view|1610593114350|    41|1610493115350|\n",
            "+-----+--------+-------------+------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Gettheprevioustimestampforeachuserid\n",
        "df=df.withColumn( \"session_id\", F.lag(\"timestamp\").over(Window.partitionBy(\"userid\").orderBy(\"timestamp\")),)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn5KbLT7y9jz",
        "outputId": "6083f5db-898a-4d9c-f90e-7f368a16680b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------+-------------+------+----------+\n",
            "| item|   event|    timestamp|userId|session_id|\n",
            "+-----+--------+-------------+------+----------+\n",
            "| blue|    view|1610494094350|    11|         1|\n",
            "| blue|    view|1610494094750|    11|         0|\n",
            "| blue|   close|1510593114312|    21|         1|\n",
            "|green|addtobag|1510593114350|    21|         0|\n",
            "|green|purchase|1610494094350|    31|         1|\n",
            "|  red|    view|1610493114350|    41|         1|\n",
            "|  red|   close|1610493115350|    41|         0|\n",
            "|  red|    view|1610593114350|    41|         1|\n",
            "+-----+--------+-------------+------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Define if the session is the1 stone(more than1800 s after the previous one)\n",
        "df1=df.withColumn(\n",
        "\"session_id\",\n",
        "F.when(F.col(\"timestamp\")-F.col(\"session_id\")<=1800,0).otherwise(1),\n",
        ")\n",
        "df1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNAPWBolzCda",
        "outputId": "cd94d215-b6e5-4698-f8e8-e70b1168cb14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------+-------------+------+----------+\n",
            "| item|   event|    timestamp|userId|session_id|\n",
            "+-----+--------+-------------+------+----------+\n",
            "| blue|    view|1610494094350|    11|         1|\n",
            "| blue|    view|1610494094750|    11|         1|\n",
            "| blue|   close|1510593114312|    21|         1|\n",
            "|green|addtobag|1510593114350|    21|         1|\n",
            "|green|purchase|1610494094350|    31|         1|\n",
            "|  red|    view|1610493114350|    41|         1|\n",
            "|  red|   close|1610493115350|    41|         1|\n",
            "|  red|    view|1610593114350|    41|         2|\n",
            "+-----+--------+-------------+------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#create a unique session id per session(same id can exists for different users)\n",
        "df2=df1.withColumn(\n",
        "\"session_id\",\n",
        "F.sum(\"session_id\").over(Window.partitionBy(\"userid\").orderBy(\"timestamp\")),\n",
        ")\n",
        "df2.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbIr3DKazFNU",
        "outputId": "3da91b11-7a27-4e28-dd49-df8fba42e983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------+-------------+------+----------+\n",
            "| item|   event|    timestamp|userId|session_id|\n",
            "+-----+--------+-------------+------+----------+\n",
            "| blue|    view|1610494094350|    11|         1|\n",
            "| blue|    view|1610494094750|    11|         1|\n",
            "| blue|   close|1510593114312|    21|         2|\n",
            "|green|addtobag|1510593114350|    21|         2|\n",
            "|green|purchase|1610494094350|    31|         3|\n",
            "|  red|    view|1610493114350|    41|         4|\n",
            "|  red|   close|1610493115350|    41|         4|\n",
            "|  red|    view|1610593114350|    41|         5|\n",
            "+-----+--------+-------------+------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#create a unique session id per session per user\n",
        "df3=df2.withColumn(\n",
        "\"session_id\",F.dense_rank().over(Window.orderBy(\"userid\",\"session_id\"))\n",
        ")\n",
        "\n",
        "df3.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.createDataFrame([\n",
        "(\"blue\",\"view\",1610494094750,11),\n",
        "(\"green\",\"addtobag\",1510593114350,21),\n",
        "(\"red\",\"close\",1610493115350,41),\n",
        "(\"blue\",\"view\",1610494094350,11),\n",
        "(\"blue\",\"close\",1510593114312,21),\n",
        "(\"red\",\"view\",1610493114350,41),\n",
        "(\"red\",\"view\",1610593114350,41),\n",
        "(\"green\",\"purchase\",1610494094350,31)\n",
        "],[\"item\",\"event\",\"timestamp\",\"userId\"])"
      ],
      "metadata": {
        "id": "dC0prf9dSyYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([('Alice', 1), ('Bob', 2), ('Carol', 3)], ['name', 'age'])\n",
        "\n",
        "# Group the data by the \"name\" column\n",
        "grouped_df = df.groupBy('name')"
      ],
      "metadata": {
        "id": "K2wOQKuLS599"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxsZwVAgTQPT",
        "outputId": "561a266f-c682-47bb-fbfb-10a30cbe3bf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|userId|\n",
            "+------+\n",
            "|    31|\n",
            "|    41|\n",
            "|    11|\n",
            "|    21|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6pgaFNUfTUTH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1i8nHoXsUah2gAFXoGoggOCBWEv7YaDqu",
      "authorship_tag": "ABX9TyP+wzGYrKFAuel24UWluiQ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}